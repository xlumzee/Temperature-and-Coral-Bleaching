{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c7fe28-99d7-4486-9f75-8013a50b30bd",
   "metadata": {},
   "source": [
    "# Scrape & Clean Guardian Newspaper Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901b109-e669-4179-8988-ae97a885b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a441e8-3396-4d07-88e7-ba2a84fbd132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from glob import glob\n",
    "from random import choice, randint\n",
    "from time import sleep\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from contexttimer import Timer\n",
    "from requests.adapters import HTTPAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25318e-88ed-48be-a663-78fa351ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "src_dir = os.path.join(PROJ_ROOT, \"src\")\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3df6ec-3769-4349-8992-36f45054aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport webscraping_utils\n",
    "from webscraping_utils import get_custom_headers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a91f2-4464-4f21-b327-ca5ad05d85ab",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ade28-1393-4bcc-8137-08b8f44ccb86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Objective\n",
    "\n",
    "Scrape and clean Guardian newspaper articles.\n",
    "\n",
    "### Output\n",
    "\n",
    "The output is a `DataFrame` containing the following columns\n",
    "\n",
    "1. `url_num`\n",
    "   - (news article) url number\n",
    "2. `url_name`\n",
    "   - `NULL`\n",
    "3. `url`\n",
    "   - web url of news article\n",
    "4. `text`\n",
    "   - raw text\n",
    "5. `char_count`\n",
    "   - approximate number of characters in raw text\n",
    "6. `sentence_count_raw`\n",
    "   - approximate number of sentences in raw text\n",
    "7. `token_count`\n",
    "   - approximate number of tokens in raw text\n",
    "8. `text_cleaned`\n",
    "   - cleaned text from paper\n",
    "9. `type`\n",
    "   - type of data source (`news_article`)\n",
    "\n",
    "### Notes About Data Privacy\n",
    "\n",
    "1. All news article texts were retrieved by web-scraping and storing the scraped texts in a `.parquet` file. All `.parquet` files are stored locally and will be deleted on November 30, 2024.\n",
    "2. Raw or processed text outputs are not shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabd1d1-5daf-4fb0-995c-ae9fc75e1cac",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc54081-6537-49ec-93ad-9ceccff08c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "aft_newsletter_advert = \"skip past newsletter promotionSign up to Afternoon UpdateFree daily newsletterOur Australian afternoon update breaks down the key stories of the day, telling you what's happening and why it mattersEnter your email address Sign upPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.after newsletter promotion\"\n",
    "down_to_earth_newsletter_advert = \"skip past newsletter promotionSign up to Down to EarthFree weekly newsletterThe planet's most important stories. Get all the week's environment news - the good, the bad and the essentialEnter your email address Sign upPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.after newsletter promotion\"\n",
    "morn_newsletter_advert = \"skip past newsletter promotionSign up to Morning MailFree daily newsletterOur Australian morning briefing breaks down the key stories of the day, telling you what's happening and why it mattersEnter your email address Sign upPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.after newsletter promotion\"\n",
    "newsletter_signup_advert = \"Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup\"\n",
    "\n",
    "urls_pages = range(65, 70+1)\n",
    "min_wait = 9\n",
    "max_wait = 18\n",
    "\n",
    "fname_processed = 'guardian_articles_cleaned'\n",
    "\n",
    "output_columns = [\n",
    "    'url_num',\n",
    "    'url_name',\n",
    "    'url',\n",
    "    'text',\n",
    "    'char_count',\n",
    "    'sentence_count_raw',\n",
    "    'token_count',\n",
    "    'text_cleaned',\n",
    "    'type',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b49daa-fad4-4dc8-8a04-3323f3f59f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(PROJ_ROOT, 'data')\n",
    "raw_data_dir = os.path.join(data_dir, 'raw')\n",
    "processed_data_dir = os.path.join(data_dir, 'processed')\n",
    "\n",
    "fpaths_urls = sorted(\n",
    "    glob(\n",
    "        os.path.join(raw_data_dir, 'guardian', 'urls', \"urls_*.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fpath_processed = os.path.join(\n",
    "    processed_data_dir,\n",
    "    f\"{fname_processed}_pgs_{min(urls_pages)}_{max(urls_pages)}.parquet\"\n",
    ")\n",
    "\n",
    "# Define list of request headers to (randomly) choose from\n",
    "headers_list = get_custom_headers_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa97466-b3b7-4244-a873-2a8c142a4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guardian_text_from_soup(soup):\n",
    "    \"\"\"Get Guardian text from soup object\"\"\"\n",
    "    mydiv = soup.find(\"div\", {\"class\": \"article-body-commercial-selector\"})\n",
    "    # print(mydiv)\n",
    "    if not mydiv:\n",
    "        mydiv = soup.find(\"div\", {\"class\": \"content__article-body\"})\n",
    "    unwanted_tweets = mydiv.findAll(\n",
    "        \"figure\", {\"class\": \"element element-tweet\"}\n",
    "    )\n",
    "    for unwanted in unwanted_tweets:\n",
    "        unwanted.extract()\n",
    "    unwanted_images = mydiv.findAll(\n",
    "        \"figure\", {\"class\": \"element element-embed\"}\n",
    "    )\n",
    "    for unwanted in unwanted_images:\n",
    "        unwanted.extract()\n",
    "    unwanted_images2 = mydiv.findAll(\n",
    "        \"figure\",\n",
    "        {\n",
    "            \"class\": (\n",
    "                \"element element-image \"\n",
    "                \"img--landscape fig--narrow-caption fig--has-shares\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    for unwanted in unwanted_images2:\n",
    "        unwanted.extract()\n",
    "    all_text = str(mydiv.text).replace(\"\\n\", \"\")\n",
    "    art_text = all_text.split(\"Topics\")[0]\n",
    "    # print(art_text)\n",
    "    return art_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1fd7d-7e9c-4717-85b4-98dc2c06dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_sort(test_string):\n",
    "    \"\"\"Sort by numeric part of string.\"\"\"\n",
    "    return list(map(int, re.findall(r'\\d+', test_string)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f15f9-cd8c-4e1d-a4db-7fded8aecdc2",
   "metadata": {},
   "source": [
    "## Load Article URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c04c0-cdf0-455e-8263-9b87db803750",
   "metadata": {},
   "source": [
    "Sort list of urls by page bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b69425-ac85-4657-9243-2db076dd35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths_urls.sort(key=numeric_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30165485-eed7-41e7-a2d7-9bec2ae74a68",
   "metadata": {},
   "source": [
    "Load URLs from required page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b61bb-d315-4cfc-921e-fdcfc14ff89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls_guardian = [\n",
    "#     'https://www.theguardian.com/environment/2024/nov/14/worlds-largest-known-coral-discovered-in-solomon-islands',\n",
    "#     'https://www.theguardian.com/environment/article/2024/jun/26/most-of-it-was-dead-scientists-discovers-one-of-great-barrier-reefs-worst-coral-bleaching-events',\n",
    "#     'https://www.theguardian.com/environment/article/2024/jul/30/as-record-heat-risks-bleaching-73-of-the-worlds-coral-reefs-scientists-ask-what-do-we-do-now',\n",
    "#     'https://www.theguardian.com/environment/2024/nov/19/graveyard-of-dead-coral-great-barrier-reef-bleaching-damage',\n",
    "#     'https://www.theguardian.com/environment/2024/apr/15/great-barrier-reef-coral-bleaching-global-heating',\n",
    "#     'https://www.theguardian.com/environment/article/2024/sep/09/sharks-deserting-coral-reefs-climate-crisis-heating-oceans-study',\n",
    "#     'https://www.theguardian.com/environment/2024/apr/17/great-barrier-reef-extreme-coral-bleaching',\n",
    "#     'https://www.theguardian.com/environment/2024/feb/28/aerial-surveys-of-great-barrier-reef-ordered-after-flights-confirm-mass-coral-bleaching',\n",
    "#     'https://www.theguardian.com/environment/2024/may/01/great-barrier-reef-coral-bleaching-crisis',\n",
    "#     'https://www.theguardian.com/environment/2023/dec/07/unprecedented-mass-coral-bleaching-expected-2024-professor-ove-hoegh-guldberg'\n",
    "# ]\n",
    "df_urls = (\n",
    "    pd.concat(\n",
    "        [pd.read_csv(f, usecols=['page', 'webUrl']) for f in fpaths_urls]\n",
    "    )\n",
    "    .query(\n",
    "        \"(webUrl.str.contains('coral')) | \"\n",
    "        \"(webUrl.str.contains('reef')) | \"\n",
    "        \"(webUrl.str.contains('algae')) | \"\n",
    "        \"(webUrl.str.contains('algal')) | \"\n",
    "        \"(webUrl.str.contains('ocean')) | \"\n",
    "        \"(webUrl.str.contains('marine')) | \"\n",
    "        \"(webUrl.str.contains('zooxanthellae')) | \"\n",
    "        \"(webUrl.str.contains('trophic')) | \"\n",
    "        \"(webUrl.str.contains('symbiosis')) | \"\n",
    "        \"(webUrl.str.contains('symbionts')) | \"\n",
    "        \"(webUrl.str.contains('anthropogenic')) | \"\n",
    "        \"(webUrl.str.contains('eutrophication'))\"\n",
    "    )\n",
    ")\n",
    "urls_guardian = (\n",
    "    df_urls\n",
    "    .query(f\"page.isin(@urls_pages)\")\n",
    "    ['webUrl']\n",
    "    .tolist()\n",
    ")\n",
    "assert urls_guardian\n",
    "print(\n",
    "    f\"Found {len(urls_guardian)} relevant article(s) out of {len(df_urls):,} \"\n",
    "    \"total articles to be scraped on pages \"\n",
    "    f\"{min(urls_pages)}-{max(urls_pages)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5980728-f93b-458c-98f5-8591be340975",
   "metadata": {},
   "source": [
    "## Scrape Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127365e-c233-429c-a7ac-6ea9e989c118",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfc0b9-3035-442f-8ae2-ffa81ba3a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "l_texts = {}\n",
    "for k, link in enumerate(urls_guardian, 1):\n",
    "    print(f\"Scraping article number {k}/{len(urls_guardian):,}, Link: {link}\")\n",
    "    # print(site, link)\n",
    "    with Timer() as t:\n",
    "        r_session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=2,\n",
    "            backoff_factor=0.1,\n",
    "            status_forcelist=[500, 502, 503, 504],\n",
    "        )\n",
    "        r_session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "        try:\n",
    "            headers = choice(headers_list)\n",
    "            page_response = r_session.get(link, timeout=5, headers=headers)\n",
    "        except Exception as ex:\n",
    "            print(f\"{ex} Error connecting to {link}\")\n",
    "        else:\n",
    "            try:\n",
    "                soup = BeautifulSoup(page_response.content, \"lxml\")\n",
    "                # print(soup.prettify())\n",
    "            except Exception as e:\n",
    "                print(f\"Experienced error {str(e)} when scraping {link}\")\n",
    "                text = np.nan\n",
    "            else:\n",
    "                text = get_guardian_text_from_soup(soup)\n",
    "    num_chars = len(text) if text else 0\n",
    "    print(\n",
    "        f\"Scraped {num_chars:,} characters of text from article {k} in \"\n",
    "        f\"{t.elapsed:.2f} seconds\"\n",
    "    )\n",
    "    l_texts[link] = [text]\n",
    "    if k != len(urls_guardian):\n",
    "        random_sleep_time = randint(min_wait, max_wait)\n",
    "        print(\n",
    "            f\"Pausing for {random_sleep_time} seconds after retrieving \"\n",
    "            f\"article {k} from pages {min(urls_pages)}-{max(urls_pages)}...\",\n",
    "            end=\"\",\n",
    "        )\n",
    "        sleep(random_sleep_time)\n",
    "        print(\"done.\")\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b5c1b-2df2-4bdd-bff2-fe5538b411e2",
   "metadata": {},
   "source": [
    "Store article metadata and text in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e20c8-3d8b-4cd8-bb0d-2bd437892bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.DataFrame.from_dict(l_texts, orient=\"index\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"url\", 0: \"text\"})\n",
    "    .assign(\n",
    "        url_num=lambda df: pd.Series(range(1, len(df)+1)),\n",
    "        url_name=None,\n",
    "        char_count=lambda df: df['text'].str.len(),\n",
    "        sentence_count_raw=lambda df: df['text'].str.split(\". \").str.len(),\n",
    "        token_count=lambda df: (df['text'].str.len()/4).round().astype(int),\n",
    "        type='news_article',\n",
    "    )\n",
    "    .convert_dtypes()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea69c95-ebdb-450a-a299-6a95fe65e29f",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b6d9f-87ec-4a27-89fd-9f036af8aff1",
   "metadata": {},
   "source": [
    "Clean the article text (in the `text` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a1ec0-5225-4848-bbab-2aa9e6b76412",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .assign(\n",
    "        text_cleaned=lambda df: (\n",
    "            df['text']\n",
    "            .str.replace(\"– video\", \" \")\n",
    "            .str.replace('–', '')\n",
    "            .str.replace('“', '\\\"')\n",
    "            .str.replace('”', '\\\"')\n",
    "            .str.replace(\"‘\", \"'\")\n",
    "            .str.replace('’', \"'\")\n",
    "            .str.replace('|', '')\n",
    "            .str.replace(aft_newsletter_advert, \" \")\n",
    "            .str.replace(down_to_earth_newsletter_advert, \" \")\n",
    "            .str.replace(morn_newsletter_advert, \" \")\n",
    "            .str.replace(newsletter_signup_advert, \"\")\n",
    "            .str.replace(\"QuickGuideWhat is coral bleaching?Show\", \" What is coral bleaching? \")\n",
    "            .str.replace(\"Quick GuideWhat is coral bleaching?Show\", \" What is coral bleaching? \")\n",
    "            .str.replace(\"Was this helpful?Thank you for your feedback.\", \" \")\n",
    "            .str.replace(\"Read more\", \" \")\n",
    "            .str.replace(\"©\", \"\")\n",
    "        )\n",
    "        \n",
    "    )\n",
    "    [output_columns]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fecba-949e-43e3-82c9-b4cd2edea59c",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a55a69-5aca-4fa6-9a7d-1d3ab66d9606",
   "metadata": {},
   "source": [
    "Export to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a476491-2fcf-425c-9dab-0ca6f30c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_parquet(fpath_processed, index=False)\n",
    "print(\n",
    "    f\"Exported {len(df):,} row(s) of article text from pages \"\n",
    "    f\"{min(urls_pages)}-{max(urls_pages)} to \"\n",
    "    f\"{os.path.basename(fpath_processed)}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
