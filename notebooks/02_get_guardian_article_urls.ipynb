{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe75fbba-7ef5-48b9-8d8a-d0f91436e588",
   "metadata": {},
   "source": [
    "# Scrape Guardian Article URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b38f0-c26e-45ea-a33f-639090ebb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962713f-db61-4d64-b8a5-1308c746d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=os.path.join(os.pardir, os.pardir, '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa171178-6635-4133-9112-2e45efec2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = os.path.join(os.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6ded5-f9fc-42cf-b3e9-ac64f3b0609c",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e481a1-e90f-4e88-9010-1df167e7f0fa",
   "metadata": {},
   "source": [
    "Retrieve URLs for Guardian newspaper articles on coral bleaching events covering the last five years (between January 1, 2019 and October 31, 2024, inclusive), by querying the [`/content` endpoint of the Guardian API](https://open-platform.theguardian.com/documentation/search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeca919-916c-4111-98ae-11e7baf92830",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674aacf-40e9-4b52-8e1f-8264a00c3773",
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_start_page_num = 1\n",
    "guardian_num_pages_wanted = 10\n",
    "guardian_query_min_delay = 8\n",
    "guardian_query_max_delay = 14\n",
    "\n",
    "url = \"https://content.guardianapis.com/search\"\n",
    "\n",
    "# API Query inputs\n",
    "query_params = {\n",
    "    \"guardian\": {\n",
    "        \"section\": \"environment\",\n",
    "        \"from-date\": \"2019-01-01\",\n",
    "        \"to-date\": \"2024-10-31\",\n",
    "        \"order-by\": \"oldest\",\n",
    "        \"page-size\": 10,\n",
    "        \"q\": \"coral bleaching event\",\n",
    "        \"api-key\": os.getenv('GUARDIAN_API_KEY'),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8b1f3-f532-41df-b7c1-081eee3f8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(PROJ_ROOT, 'data')\n",
    "raw_data_dir = os.path.join(data_dir, 'raw', 'guardian', 'urls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbeaf3-e4b7-4239-a118-40d8f48b2cb9",
   "metadata": {},
   "source": [
    "## Retrieve Guardian Newspaper Metadata from API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704ed3a-414b-4cf5-a714-665769bdbd20",
   "metadata": {},
   "source": [
    "Perform the following workflow\n",
    "\n",
    "1. step 1. Calculate the maximum number of pages of results available based on the search term\n",
    "2. step 2. Set the maximum page number to be queried\n",
    "\n",
    "Loop over each page to be queried and retrieve article details for all articles on that page\n",
    "1. step 3. Send GET request to API endpoint and retrieve the response\n",
    "2. step 4. Get results dictionary from the response attribute of the `json`ified response\n",
    "3. step 5. Extract attributes of response json and store in dictionary\n",
    "4. step 6. Convert dictionary of urls to a `DataFrame`\n",
    "5. step 7. Append page number of `DataFrame` from step 6.\n",
    "6. step 8. Concatenate `DataFrame`s across all pages of search results\n",
    "7. step 9. Filter `DataFrame` to retain published news articles and remove blog posts\n",
    "8. step 10. Export `DataFrame` of metadata to `*.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56861e-a8b6-4dbb-8448-b4f7facbc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Guardian urls to file\n",
    "dfs_guardian_details = []\n",
    "# 1. Find maximum number of pages of results available\n",
    "guardian_max_pages_returned = requests.get(\n",
    "    url, params=query_params[\"guardian\"]\n",
    ").json()[\"response\"][\"pages\"]\n",
    "print(f\"Found {guardian_max_pages_returned} pages of results\")\n",
    "# 2. Set the maximum page number to be queried\n",
    "if guardian_num_pages_wanted == -1:\n",
    "    guardian_max_page_num = guardian_max_pages_returned\n",
    "    guardian_pages_to_use = \"all available\"\n",
    "else:\n",
    "    guardian_max_page_num = guardian_start_page_num + guardian_num_pages_wanted\n",
    "    guardian_pages_to_use = \"requested\"\n",
    "print(\n",
    "    f\"Retrieving articles from {guardian_pages_to_use} pages, \"\n",
    "    f\"number of requested pages = {guardian_num_pages_wanted}\\n\"\n",
    ")\n",
    "# Loop over all pages to be queried and retrieve article details\n",
    "for page in range(guardian_start_page_num, guardian_max_page_num):\n",
    "    d = {}\n",
    "    query_params[\"guardian\"][\"page\"] = page\n",
    "    # 3. Send GET request to API and retrieve response\n",
    "    r = requests.get(url, params=query_params[\"guardian\"])\n",
    "    assert r.status_code == 200\n",
    "    # print(r.json().keys())\n",
    "    # 4. Get results dict from response attribute of jsonified response\n",
    "    try:\n",
    "        rdocs = r.json()[\"response\"][\"results\"]\n",
    "    except KeyError as e:\n",
    "        # print(r.json())\n",
    "        if (\n",
    "            r.json()[\"response\"][\"message\"]\n",
    "            == \"requested page is beyond the number of available pages\"\n",
    "        ):\n",
    "            print(f\"Page {page} exceeded number of available pages. Stopping.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Page: {page}, Found: {len(rdocs)} articles\")\n",
    "        # 5. Extract various attributes (metadata) of response json and store in dict\n",
    "        for key in [\n",
    "            \"webUrl\",\n",
    "            \"id\",\n",
    "            \"webPublicationDate\",\n",
    "            \"apiUrl\",\n",
    "            \"webTitle\",\n",
    "            \"document_type\",\n",
    "            \"sectionId\",\n",
    "            \"sectionName\",\n",
    "            \"type\",\n",
    "            \"isHosted\",\n",
    "            \"pillarId\",\n",
    "            \"pillarName\",\n",
    "        ]:\n",
    "            d[key] = []\n",
    "            for rr in rdocs:\n",
    "                try:\n",
    "                    rr[key]\n",
    "                    d[key].append(rr[key])\n",
    "                except Exception as e:\n",
    "                    d[key].append(None)\n",
    "        print(f\"Retrieved {len(rdocs)} article details from page {page}\")\n",
    "        # 6. Convert dict of urls to DataFrame of urls\n",
    "        df_guardian_article = pd.DataFrame.from_dict(d, orient=\"index\").T\n",
    "        # 7. Append page number of DataFrame\n",
    "        df_guardian_article[\"page\"] = page\n",
    "        dfs_guardian_details.append(df_guardian_article)\n",
    "        # Pause between pages\n",
    "        if page != (guardian_start_page_num + guardian_num_pages_wanted) - 1:\n",
    "            random_sleep_time = randint(\n",
    "                guardian_query_min_delay, guardian_query_max_delay\n",
    "            )\n",
    "            print(\n",
    "                f\"Pausing for {random_sleep_time} seconds before retrieving \"\n",
    "                f\"from page {page+1}\\n\"\n",
    "            )\n",
    "            sleep(random_sleep_time)\n",
    "# 8. Concatenate DataFrames across all pages\n",
    "df_guardian_details = pd.concat(\n",
    "    dfs_guardian_details, axis=0, ignore_index=True\n",
    ").drop_duplicates()\n",
    "# 9. Filter DataFrame to retain articles and remove blogs\n",
    "df_guardian_details = df_guardian_details.loc[\n",
    "    (df_guardian_details[\"type\"] == \"article\")\n",
    "    & (~df_guardian_details[\"webUrl\"].str.contains(\"blog\"))\n",
    "]\n",
    "print(f\"\\nGot {df_guardian_details.shape[0]:,} articles after filtering\")\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_guardian_details)\n",
    "# 10. Export DataFrame of metadata to *.csv file\n",
    "fname_urls = (\n",
    "    f\"urls_pgs_{guardian_start_page_num}_{guardian_max_page_num-1}.csv\"\n",
    ")\n",
    "fpath_urls = os.path.join(raw_data_dir, fname_urls)\n",
    "df_guardian_details.to_csv(fpath_urls, index=False)\n",
    "print(f\"Exported {len(df_guardian_details):,} rows to {fname_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21f424-fa22-42e7-967c-79eba1c09667",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f112c-31b2-43dc-9224-391a2055a6d1",
   "metadata": {},
   "source": [
    "This notebook has retrieved URLs for news articles on *coral bleaching events* (search term) in the Guardian's **Environment** section over the last five years (between Jan 1, 2019 and October 31, 2024).\n",
    "\n",
    "These search results were returned on a single batch of pages from the Guardian API. The URLs in the query results were returned in response to the search term *coral bleaching events*.\n",
    "\n",
    "The batch size is specified by `guardian_num_pages_wanted`. The batch starts at a page number specified by `guardian_start_page_num` and ends at a page number that is calculated from `guardian_start_page_num + guardian_num_pages_wanted`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
