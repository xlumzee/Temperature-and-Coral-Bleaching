{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c7fe28-99d7-4486-9f75-8013a50b30bd",
   "metadata": {},
   "source": [
    "# Scrape Guardian Newspaper Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901b109-e669-4179-8988-ae97a885b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a441e8-3396-4d07-88e7-ba2a84fbd132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from glob import glob\n",
    "from random import choice, randint\n",
    "from time import sleep\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from contexttimer import Timer\n",
    "from requests.adapters import HTTPAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25318e-88ed-48be-a663-78fa351ea358",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = os.path.join(os.pardir)\n",
    "src_dir = os.path.join(PROJ_ROOT, \"src\")\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3df6ec-3769-4349-8992-36f45054aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport webscraping_utils\n",
    "from webscraping_utils import get_custom_headers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a91f2-4464-4f21-b327-ca5ad05d85ab",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ade28-1393-4bcc-8137-08b8f44ccb86",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Scrape Guardian newspaper articles from URLs that were exported in the previous notebook (`02_get_guardian_article_urls.ipynb`).\n",
    "\n",
    "### Output\n",
    "\n",
    "The output is a `DataFrame` containing the following columns\n",
    "\n",
    "1. `url_num`\n",
    "   - (news article) url number\n",
    "2. `url_name`\n",
    "   - `NULL`\n",
    "3. `url`\n",
    "   - web url of news article\n",
    "4. `text`\n",
    "   - raw text\n",
    "5. `char_count`\n",
    "   - approximate number of characters in raw text\n",
    "6. `sentence_count_raw`\n",
    "   - approximate number of sentences in raw text\n",
    "7. `token_count`\n",
    "   - approximate number of tokens in raw text\n",
    "8. `type`\n",
    "   - type of data source (`news_article`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabd1d1-5daf-4fb0-995c-ae9fc75e1cac",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc54081-6537-49ec-93ad-9ceccff08c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title_search_terms = [\n",
    "    'coral',\n",
    "    'reef',\n",
    "    'algae',\n",
    "    'algal',\n",
    "    'ocean',\n",
    "    'marine',\n",
    "    'zooxanthellae',\n",
    "    'trophic',\n",
    "    'symbiosis',\n",
    "    'symbionts',\n",
    "    'anthropogenic',\n",
    "    'eutrophication',\n",
    "]\n",
    "\n",
    "urls_pages = range(1, 5+1)\n",
    "min_wait = 9\n",
    "max_wait = 18\n",
    "\n",
    "fname_processed = 'guardian_articles'\n",
    "\n",
    "output_columns = [\n",
    "    'url_num',\n",
    "    'url_name',\n",
    "    'url',\n",
    "    'text',\n",
    "    'char_count',\n",
    "    'sentence_count_raw',\n",
    "    'token_count',\n",
    "    'type',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b49daa-fad4-4dc8-8a04-3323f3f59f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(PROJ_ROOT, 'data')\n",
    "raw_data_dir = os.path.join(data_dir, 'raw')\n",
    "processed_data_dir = os.path.join(data_dir, 'processed')\n",
    "\n",
    "fpaths_urls = sorted(\n",
    "    glob(\n",
    "        os.path.join(raw_data_dir, 'guardian', 'urls', \"urls_*.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fpath_processed = os.path.join(\n",
    "    processed_data_dir,\n",
    "    f\"{fname_processed}_pgs_{min(urls_pages)}_{max(urls_pages)}.parquet\"\n",
    ")\n",
    "\n",
    "# Define list of request headers to (randomly) choose from\n",
    "headers_list = get_custom_headers_list()\n",
    "\n",
    "# combine article search term list into string with | as delimiter\n",
    "article_title_search_terms_str = ' | '.join(\n",
    "    [f\"(webUrl.str.contains('{t}'))\" for t in article_title_search_terms]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa97466-b3b7-4244-a873-2a8c142a4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_sort(test_string):\n",
    "    \"\"\"Sort by numeric part of string.\"\"\"\n",
    "    return list(map(int, re.findall(r'\\d+', test_string)))[0]\n",
    "\n",
    "\n",
    "def get_guardian_text_from_soup(soup):\n",
    "    \"\"\"Get Guardian text from beautifulsoup4 soup object\"\"\"\n",
    "    mydiv = soup.find(\"div\", {\"class\": \"article-body-commercial-selector\"})\n",
    "    # print(mydiv)\n",
    "    if not mydiv:\n",
    "        mydiv = soup.find(\"div\", {\"class\": \"content__article-body\"})\n",
    "    unwanted_tweets = mydiv.findAll(\n",
    "        \"figure\", {\"class\": \"element element-tweet\"}\n",
    "    )\n",
    "    for unwanted in unwanted_tweets:\n",
    "        unwanted.extract()\n",
    "    unwanted_images = mydiv.findAll(\n",
    "        \"figure\", {\"class\": \"element element-embed\"}\n",
    "    )\n",
    "    for unwanted in unwanted_images:\n",
    "        unwanted.extract()\n",
    "    unwanted_images2 = mydiv.findAll(\n",
    "        \"figure\",\n",
    "        {\n",
    "            \"class\": (\n",
    "                \"element element-image \"\n",
    "                \"img--landscape fig--narrow-caption fig--has-shares\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    for unwanted in unwanted_images2:\n",
    "        unwanted.extract()\n",
    "    all_text = str(mydiv.text).replace(\"\\n\", \"\")\n",
    "    art_text = all_text.split(\"Topics\")[0]\n",
    "    return art_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f15f9-cd8c-4e1d-a4db-7fded8aecdc2",
   "metadata": {},
   "source": [
    "## Load Article URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c04c0-cdf0-455e-8263-9b87db803750",
   "metadata": {},
   "source": [
    "Sort list of urls by page bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b69425-ac85-4657-9243-2db076dd35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths_urls.sort(key=numeric_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30165485-eed7-41e7-a2d7-9bec2ae74a68",
   "metadata": {},
   "source": [
    "Load and filter URLs from required page to only get URLs whose artitle title contains terms related to *coral bleaching*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b61bb-d315-4cfc-921e-fdcfc14ff89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = (\n",
    "    pd.concat(\n",
    "        [pd.read_csv(f, usecols=['page', 'webUrl']) for f in fpaths_urls]\n",
    "    )\n",
    "    .query(article_title_search_terms_str)\n",
    ")\n",
    "urls_guardian = (\n",
    "    df_urls\n",
    "    .query(f\"page.isin(@urls_pages)\")\n",
    "    ['webUrl']\n",
    "    .tolist()\n",
    ")\n",
    "assert urls_guardian\n",
    "print(\n",
    "    f\"Found {len(urls_guardian)} relevant article(s) out of {len(df_urls):,} \"\n",
    "    \"total articles to be scraped on pages \"\n",
    "    f\"{min(urls_pages)}-{max(urls_pages)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd79433-ee10-41cf-bc1c-d938043b7e9f",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. `.query(article_title_search_terms_str)` evaluates to\n",
    "   ```python\n",
    "   .query(\n",
    "       \"(webUrl.str.contains('coral')) | \"\n",
    "       \"(webUrl.str.contains('reef')) | \"\n",
    "       \"(webUrl.str.contains('algae')) | \"\n",
    "       \"(webUrl.str.contains('algal')) | \"\n",
    "       \"(webUrl.str.contains('ocean')) | \"\n",
    "       \"(webUrl.str.contains('marine')) | \"\n",
    "       \"(webUrl.str.contains('zooxanthellae')) | \"\n",
    "       \"(webUrl.str.contains('trophic')) | \"\n",
    "       \"(webUrl.str.contains('symbiosis')) | \"\n",
    "       \"(webUrl.str.contains('symbionts')) | \"\n",
    "       \"(webUrl.str.contains('anthropogenic')) | \"\n",
    "       \"(webUrl.str.contains('eutrophication'))\"\n",
    "   )\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5980728-f93b-458c-98f5-8591be340975",
   "metadata": {},
   "source": [
    "## Scrape Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127365e-c233-429c-a7ac-6ea9e989c118",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfc0b9-3035-442f-8ae2-ffa81ba3a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "l_texts = {}\n",
    "for k, link in enumerate(urls_guardian, 1):\n",
    "    print(f\"Scraping article number {k}/{len(urls_guardian):,}, Link: {link}\")\n",
    "    with Timer() as t:\n",
    "        # construct session\n",
    "        r_session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=2,\n",
    "            backoff_factor=0.1,\n",
    "            status_forcelist=[500, 502, 503, 504],\n",
    "        )\n",
    "        r_session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "        # scrape\n",
    "        try:\n",
    "            # construct headers dictionary\n",
    "            headers = choice(headers_list)\n",
    "            page_response = r_session.get(link, timeout=5, headers=headers)\n",
    "        except Exception as ex:\n",
    "            print(f\"{ex} Error connecting to {link}\")\n",
    "        else:\n",
    "            try:\n",
    "                # extract entire soup from raw scraped text\n",
    "                soup = BeautifulSoup(page_response.content, \"lxml\")\n",
    "                # print(soup.prettify())\n",
    "            except Exception as e:\n",
    "                print(f\"Experienced error {str(e)} when scraping {link}\")\n",
    "                text = np.nan\n",
    "            else:\n",
    "                # parse output text from relevant parts of soup\n",
    "                text = get_guardian_text_from_soup(soup)\n",
    "    num_chars = len(text) if text else 0\n",
    "    print(\n",
    "        f\"Scraped {num_chars:,} characters of text from article {k} in \"\n",
    "        f\"{t.elapsed:.2f} seconds\"\n",
    "    )\n",
    "    # append article URL and parsed text from that URL to dictionary\n",
    "    l_texts[link] = [text]\n",
    "    # if more articles are to be scraped then print error message and wait\n",
    "    if k != len(urls_guardian):\n",
    "        random_sleep_time = randint(min_wait, max_wait)\n",
    "        print(\n",
    "            f\"Pausing for {random_sleep_time} seconds after retrieving \"\n",
    "            f\"article {k} from pages {min(urls_pages)}-{max(urls_pages)}...\",\n",
    "            end=\"\",\n",
    "        )\n",
    "        sleep(random_sleep_time)\n",
    "        print(\"done.\")\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1cfe1e-06e0-4a65-bae6-99f8f12b6639",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b5c1b-2df2-4bdd-bff2-fe5538b411e2",
   "metadata": {},
   "source": [
    "Store article metadata and text in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e20c8-3d8b-4cd8-bb0d-2bd437892bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    # construct DataFrame from scraped dicrionary\n",
    "    pd.DataFrame.from_dict(l_texts, orient=\"index\")\n",
    "    .reset_index()\n",
    "    # rename columns\n",
    "    .rename(columns={\"index\": \"url\", 0: \"text\"})\n",
    "    # extract metadata\n",
    "    .assign(\n",
    "        url_num=lambda df: pd.Series(range(1, len(df)+1)),\n",
    "        url_name=None,\n",
    "        # rough count of text characters\n",
    "        char_count=lambda df: df['text'].str.len(),\n",
    "        # rough sentence count\n",
    "        sentence_count_raw=lambda df: df['text'].str.split(\". \").str.len(),\n",
    "        # rough token count\n",
    "        token_count=lambda df: (df['text'].str.len()/4).round().astype(int),\n",
    "        type='news_article',\n",
    "    )\n",
    "    .convert_dtypes()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fecba-949e-43e3-82c9-b4cd2edea59c",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a55a69-5aca-4fa6-9a7d-1d3ab66d9606",
   "metadata": {},
   "source": [
    "Export to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a476491-2fcf-425c-9dab-0ca6f30c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_parquet(fpath_processed, index=False)\n",
    "print(\n",
    "    f\"Exported {len(df):,} row(s) of article text from pages \"\n",
    "    f\"{min(urls_pages)}-{max(urls_pages)} to \"\n",
    "    f\"{os.path.basename(fpath_processed)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a36521-0253-4299-b8f6-f522b598f446",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443d859-d7f3-4596-80fa-31d4307684cb",
   "metadata": {},
   "source": [
    "This notebook has retrieved the text of news articles in the Guardian's **Environment** section. Scraped articles contained a pre-determined set of terms related to *coral bleaching* events. Text was retrieved from batches of articles at once, where each batch corresponds to a single page of results returned by quering the publication's API `/content` endpoint.\n",
    "\n",
    "In order to ensure articles discussed coral bleaching, each scraped article retrieved contained one of the following terms\n",
    "\n",
    "1. coral\n",
    "2. reef\n",
    "3. algae\n",
    "4. algal\n",
    "5. ocean\n",
    "6. marine\n",
    "7. zooxanthellae\n",
    "8. trophic\n",
    "9. symbiosis\n",
    "10. symbionts\n",
    "11. anthropogenic\n",
    "12. eutrophication\n",
    "\n",
    "Articles that were returned by the API query but that did not contain at least one of these terms were not scraped."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
