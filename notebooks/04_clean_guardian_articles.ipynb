{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b882bf0-f679-43b6-9ac7-1fde6b8178f0",
   "metadata": {},
   "source": [
    "# Clean Text of Guardian Newspaper Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808a77d-0d52-428a-a427-7caa757ee356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "from contexttimer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5da128-157f-4fb6-9a20-9e3fc7fa0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = os.path.join(os.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6db61-2b24-47a0-b71c-943b1539f441",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a157cb-af2d-4838-892d-4ee90a46c41e",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Clean text of all scraped newspaper articles, combine with article metadata (extracted from API results) and produce a single file with all cleaned texts and associated metadata.\n",
    "\n",
    "### Output\n",
    "\n",
    "The output is a `DataFrame` containing the same columns as in the previous notebook, but with a cleaned text column\n",
    "\n",
    "1. `url_num`\n",
    "   - (news article) url number\n",
    "2. `url_name`\n",
    "   - `NULL`\n",
    "3. `url`\n",
    "   - web url of news article\n",
    "4. `text`\n",
    "   - raw text\n",
    "5. `char_count`\n",
    "   - approximate number of characters in raw text\n",
    "6. `sentence_count_raw`\n",
    "   - approximate number of sentences in raw text\n",
    "7. `token_count`\n",
    "   - approximate number of tokens in raw text\n",
    "8. `text_cleaned`<sup>[1](#myfootnote1)</sup>\n",
    "   - cleaned text from paper\n",
    "9. `type`\n",
    "   - type of data source (`news_article`)\n",
    "10. `webPublicationDate`<sup>[1](#myfootnote1)</sup>\n",
    "    - date on which news article was published\n",
    "11. `year`<sup>[1](#myfootnote1)</sup>\n",
    "    - year in which news article was published\n",
    "12. `webTitle`<sup>[1](#myfootnote1)</sup>\n",
    "    - title of published news article\n",
    "13. `page`<sup>[1](#myfootnote1)</sup>\n",
    "    - page of API results in which the article URL was listed\n",
    "\n",
    "<a name=\"myfootnote1\">1</a>: added in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f2348-a469-44aa-bc4f-c71682f52db0",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e108cbe-a761-4595-9ca0-82e0a8736ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_title_search_terms = [\n",
    "    'coral',\n",
    "    'reef',\n",
    "    'algae',\n",
    "    'algal',\n",
    "    'ocean',\n",
    "    'marine',\n",
    "    'zooxanthellae',\n",
    "    'trophic',\n",
    "    'symbiosis',\n",
    "    'symbionts',\n",
    "    'anthropogenic',\n",
    "    'eutrophication',\n",
    "]\n",
    "\n",
    "# content of banners to be removed\n",
    "latest_news = (\n",
    "    'Quick GuideHow to get the latest news from Guardian AustraliaShow Emailsign up for ourdaily morning and afternoon email newslettersAppdownload our free appand never miss the biggest storiesSocialfollow us on YouTube,TikTok,Instagram,FacebookorTwitterPodcastlisten to our daily episodes onApple Podcasts,Spotifyor search \"Full Story\" in your favourite appPhotograph Tim Robberts/Stone RF'\n",
    ")\n",
    "more_features = (\n",
    "    'Find more age of extinction coverage here, and follow biodiversity reporters Phoebe Weston and Patrick Greenfield on Twitter for all the latest news and features'\n",
    ")\n",
    "twitter_url1 = 'https//t.co/0mTKbg4pYr pic.twitter.com/tV152DUK4h'\n",
    "twitter_url2 = 'pic.twitter.com7eZb2ZXFT0— Tanya Plibersek (tanya_plibersek) July 1, 2022'\n",
    "\n",
    "aft_newsletter_advert = \"skip past newsletter promotionSign up to Afternoon UpdateFree daily newsletterOur Australian afternoon update breaks down the key stories of the day, telling you what's happening and why it mattersEnter your email address Sign upPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.after newsletter promotion\"\n",
    "down_to_earth_newsletter_advert = \"skip past newsletter promotionSign up to Down to EarthFree weekly newsletterThe planet's most important stories. Get all the week's environment news - the good, the bad and the essentialEnter your email address Sign upPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.after newsletter promotion\"\n",
    "morn_newsletter_advert = \"skip past newsletter promotionSign up to Morning MailFree daily newsletterOur Australian morning briefing breaks down the key stories of the day, telling you what's happening and why it mattersEnter your email address Sign upPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.after newsletter promotion\"\n",
    "newsletter_signup_advert = \"Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup\"\n",
    "\n",
    "# index of article that is to be spot-checked after cleaning\n",
    "rows = [\n",
    "    5,\n",
    "    18,\n",
    "    44,\n",
    "    151,\n",
    "    200,\n",
    "    229,\n",
    "    100,\n",
    "    125,\n",
    "    175,\n",
    "    250,\n",
    "    260,\n",
    "    40,\n",
    "    50,\n",
    "    70,\n",
    "    130,\n",
    "    190,\n",
    "    240,\n",
    "]\n",
    "\n",
    "output_columns = [\n",
    "    'url_num',\n",
    "    'url_name',\n",
    "    'url',\n",
    "    'text',\n",
    "    'char_count',\n",
    "    'sentence_count_raw',\n",
    "    'token_count',\n",
    "    'type',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367e5cd-0dcf-42af-94eb-8547445cacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_sort(test_string):\n",
    "    \"\"\"Sort by numeric part of string.\"\"\"\n",
    "    return list(map(int, re.findall(r'\\d+', test_string)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46abba58-c806-4652-93a4-4395809fa47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(PROJ_ROOT, 'data')\n",
    "raw_data_dir = os.path.join(data_dir, 'raw')\n",
    "processed_data_dir = os.path.join(data_dir, 'processed')\n",
    "\n",
    "# filepaths of scraped pages with articles\n",
    "fpaths_processed = glob(\n",
    "    os.path.join(processed_data_dir, \"guardian_articles*pg*.parquet\")\n",
    ")\n",
    "# sort by page number of filename\n",
    "fpaths_processed.sort(key=numeric_sort)\n",
    "\n",
    "# filepaths of URLs retrieved from API results\n",
    "fpaths_urls = sorted(\n",
    "    glob(\n",
    "        os.path.join(raw_data_dir, 'guardian', 'urls', \"urls_*.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# filepath at which to save cleaned data\n",
    "fpath_processed_cleaned_and_combined = os.path.join(\n",
    "    processed_data_dir, \"guardian_articles_cleaned_combined.parquet\"\n",
    ")\n",
    "\n",
    "# combine article search term list into string with | as delimiter\n",
    "article_title_search_terms_str = ' | '.join(\n",
    "    [f\"(webUrl.str.contains('{t}'))\" for t in article_title_search_terms]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954211e2-5d44-4ccb-8d9f-873258bb52e5",
   "metadata": {},
   "source": [
    "## Clean Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f12f6-d3c5-48e2-9817-4ecac8f8450c",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87217b5-80bc-4a15-a63f-caea199adf22",
   "metadata": {},
   "source": [
    "#### Scraped Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc51ec2-c7f0-4235-8bdb-ebc944f83243",
   "metadata": {},
   "source": [
    "Load all columns of scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec986474-4a29-49fa-b53b-b77d8926559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(fpaths_processed, columns=output_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87b48b-53f5-4a96-844c-37c9e63dade1",
   "metadata": {},
   "source": [
    "There was some overlap between articles returned from different pages of the API. This can be detected based on the presence of duplicates in the `url` column and is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afda53-360d-44d5-876a-c7b50381efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Number of rows: {len(df)}, \"\n",
    "    \"Number of rows without duplicates: \"\n",
    "    f\"{len(df.drop_duplicates(subset=['url']))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf964d9-a6b6-42e3-ade8-ae4e3428bfc8",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. When specifying [API inputs](https://open-platform.theguardian.com/explore/), the `order-by` filter was set to ensure the order of the results returned by querying the API was deterministic across multiple queries. Despite this, there is some overlap between links shown on different pages of API response results. It is not known why this occurred. So, some articles were returned on multiple pages of API response results. This introduced the duplication shown above. Future work should start by checking if the API response returns the same listings across multiple calls to the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f4b0a-6322-4bba-86c8-0c8cd6cef3b0",
   "metadata": {},
   "source": [
    "For this reason, rows with duplicated articles have to be dropped based on the URL column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037f408-b276-47ca-a8d8-797243638431",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['url'])\n",
    "print(\n",
    "    f\"Number of rows: {len(df)}, \"\n",
    "    \"Number of rows without duplicates: \"\n",
    "    f\"{len(df.drop_duplicates(subset=['url']))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4d8fc-d46e-492c-aac7-8163c6b655d1",
   "metadata": {},
   "source": [
    "#### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324ec3f-17ac-4613-b303-65eac3b9763b",
   "metadata": {},
   "source": [
    "Load and process metadata from list of URLs retrieved by querying the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe911d2-94db-421b-a96e-fa768dcf2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = (\n",
    "    # extract\n",
    "    pd.concat(\n",
    "        [pd.read_csv(f) for f in fpaths_urls]\n",
    "    )\n",
    "    # # filter to get articles related to coral bleaching\n",
    "    .query(article_title_search_terms_str)\n",
    "    # transform\n",
    "    # # add datetime attributes\n",
    "    .assign(\n",
    "        webPublicationDate=lambda df: pd.to_datetime(\n",
    "            df['webPublicationDate']\n",
    "        ),\n",
    "        year=lambda df: df['webPublicationDate'].dt.year,\n",
    "    )\n",
    "    # # slice to get useful columns\n",
    "    [['webUrl', 'webPublicationDate', 'year', 'webTitle', 'page']]\n",
    ")\n",
    "df_urls.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207e680-e4ce-4f4f-bcf6-e5640cd46086",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. `.query(article_title_search_terms_str)` evaluates to\n",
    "   ```python\n",
    "   .query(\n",
    "       \"(webUrl.str.contains('coral')) | \"\n",
    "       \"(webUrl.str.contains('reef')) | \"\n",
    "       \"(webUrl.str.contains('algae')) | \"\n",
    "       \"(webUrl.str.contains('algal')) | \"\n",
    "       \"(webUrl.str.contains('ocean')) | \"\n",
    "       \"(webUrl.str.contains('marine')) | \"\n",
    "       \"(webUrl.str.contains('zooxanthellae')) | \"\n",
    "       \"(webUrl.str.contains('trophic')) | \"\n",
    "       \"(webUrl.str.contains('symbiosis')) | \"\n",
    "       \"(webUrl.str.contains('symbionts')) | \"\n",
    "       \"(webUrl.str.contains('anthropogenic')) | \"\n",
    "       \"(webUrl.str.contains('eutrophication'))\"\n",
    "   )\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade1ada-319d-47c7-80e5-7c5da055d3f7",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164dd438-ee5e-4b01-b040-49c8dcf4070d",
   "metadata": {},
   "source": [
    "Specify index of `DataFrame` row whose text is to be printed after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fb722-0fe8-4101-a1cd-f36325425e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de244e33-7639-406f-b3fa-a9de7c779606",
   "metadata": {},
   "source": [
    "Perform cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49911de-625a-49a7-8a48-585a50c11b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .assign(\n",
    "        text_cleaned=lambda df: (\n",
    "            df['text']\n",
    "            # FIRST PASS\n",
    "            # new line was dropped after web scraping, so there is no trailing\n",
    "            # space after '.' This regex adds this trailing space at end of\n",
    "            # each sentence.\n",
    "            .str.replace('\\.(?!\\s|\\d|$)', '. ', regex=True)\n",
    "            # remove text corresponding to video\n",
    "            .str.replace(\"– video\", \" \")\n",
    "            # # old\n",
    "            # .str.replace('–', \"\")\n",
    "            # new\n",
    "            # remove special characters\n",
    "            .str.replace('–', \"-\")\n",
    "            .str.replace('“', '\\\"')\n",
    "            .str.replace('”', '\\\"')\n",
    "            .str.replace(\"‘\", \"'\")\n",
    "            .str.replace('’', \"'\")\n",
    "            .str.replace('|', '')\n",
    "            # remove advertising banners\n",
    "            .str.replace(aft_newsletter_advert, \" \")\n",
    "            .str.replace(down_to_earth_newsletter_advert, \" \")\n",
    "            .str.replace(morn_newsletter_advert, \" \")\n",
    "            .str.replace(newsletter_signup_advert, \"\")\n",
    "            # remove unwanted part of embedded infographic\n",
    "            .str.replace(\n",
    "                \"QuickGuideWhat is coral bleaching?Show\",\n",
    "                \" What is coral bleaching? \"\n",
    "            )\n",
    "            .str.replace(\n",
    "                \"Quick GuideWhat is coral bleaching?Show\",\n",
    "                \" What is coral bleaching? \"\n",
    "            )\n",
    "            .str.replace(\"Was this helpful?Thank you for your feedback.\", \" \")\n",
    "            .str.replace(\"Read more\", \" \")\n",
    "            # remove special characters\n",
    "            .str.replace(\"©\", \"\")\n",
    "            # SECOND PASS\n",
    "            # remove special characters\n",
    "            .str.replace(\":\", \"\")\n",
    "            .str.replace(\",\", \" \")\n",
    "            # remove double white-space\n",
    "            .str.replace(\"  \", \" \")\n",
    "            # remove special characters\n",
    "            .str.replace(\"\\xa0\", \"\")\n",
    "            # remove adververtising banners\n",
    "            .str.replace(latest_news, '')\n",
    "            # remove special characters\n",
    "            .str.replace(\"@\", \"\")\n",
    "            .str.replace(\"\\'\", \"\")\n",
    "            .str.replace('\\\"', \"\")\n",
    "            # remove social meida hashtags\n",
    "            .str.replace(\"#coralbleaching2020\", '')\n",
    "            .str.replace(\"#GreatBarrierReef\", '')\n",
    "            # remove urls\n",
    "            .str.replace(twitter_url1, ' ')\n",
    "            .str.replace(\"#pic.twitter.com/Tz1vqfI40t\", '')\n",
    "            # add hyphen\n",
    "            .str.replace('20122013', '2012-2013')\n",
    "            # remove text corresponding to embedded images\n",
    "            .str.replace(\"The Guardian\", \"\")\n",
    "            # remove special characters\n",
    "            .str.replace(\"/\", '')\n",
    "            .str.replace(\"…\", '')\n",
    "            # remove text corresponding to embedded images\n",
    "            .str.replace('Photograph Manu San FelixNGKAUST', ' ')\n",
    "            # remove text to expand size of embedded image\n",
    "            .str.replace('View image in full', ' ')\n",
    "            # remove adververtising banners\n",
    "            .str.replace(more_features, '')\n",
    "            # # remove text corresponding to embedded images\n",
    "            .str.replace('Photograph Serço Ekşiyan', '')\n",
    "            .str.replace('Photograph Sam McNeilAP', '')\n",
    "            # remove urls\n",
    "            .str.replace(twitter_url2, '')\n",
    "            .str.replace('httpst.coeE5LCrSwtL— Terry Hughes (ProfTerryHughes)', ' ')\n",
    "            # remove text corresponding to embedded images\n",
    "            .str.replace('Photograph Grumpy Turtle Films', ' ')\n",
    "            # remove narrative text\n",
    "            .str.split(\"As told to\", expand=True)[0]\n",
    "            # change to lowercase\n",
    "            .str.lower()\n",
    "        )\n",
    "    )\n",
    "    [output_columns[:-1]+ ['text_cleaned'] + [output_columns[-1]]]\n",
    ")\n",
    "# (optional) print cleaned text to assess cleaning procedure\n",
    "row = df.iloc[rows[k]]\n",
    "url = row['url']\n",
    "text = row['text_cleaned']\n",
    "# print(url)\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a1633-342f-42a9-98ff-a6be85100cf7",
   "metadata": {},
   "source": [
    "Show the first row of the combined `DataFrame` with the `text_cleaned` (cleaned text) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d3267-0b92-4aae-a77a-a7123c275578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6624c-b0aa-4cd7-ad31-7d92bd4dae20",
   "metadata": {},
   "source": [
    "Show the first row of the combined `DataFrame` with the article metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68c63f-90fb-429a-9fc3-6468cd72e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d421a9-0406-452b-b858-a7da56e6e8a1",
   "metadata": {},
   "source": [
    "Merge article text and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6605c-f3d9-4f8f-8e19-5704f962947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df.merge(\n",
    "    df_urls.rename(columns={\"webUrl\": \"url\"}), on=['url'], how='left',\n",
    ")\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_merged.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa991a0-eb58-41cb-a868-25a240655024",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e45db-ace1-45f0-a93f-484c8efc93aa",
   "metadata": {},
   "source": [
    "Export merged article contents to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692848c9-99b7-4eae-bd2c-a01e7df1ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_merged.to_parquet(fpath_processed_cleaned_and_combined, index=False)\n",
    "print(\n",
    "    f\"Exported {len(df):,} rows of data with cleaned article texts to \"\n",
    "    f\"{os.path.basename(fpath_processed_cleaned_and_combined)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a9619-9e0e-4fdd-94a4-92aa357f8f6b",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe15646-da22-47db-b4d1-a71551aae30e",
   "metadata": {},
   "source": [
    "This notebook has cleaned the text of the scraped batches of Guardian news articles relating to *coral bleaching*. All cleaned texts were then combined with article metadata retrieved from querying the publication's `/content` API endpoint and then exported to a single file for use in NLP analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
